{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN using Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution functions, including:\n",
    "    - Zero Padding\n",
    "    - Convolve window \n",
    "    - Convolution forward\n",
    "    - Convolution backward\n",
    "### Pooling functions, including:\n",
    "    - Pooling forward\n",
    "    - Create mask \n",
    "    - Distribute value\n",
    "    - Pooling backward\n",
    "### Input data:\n",
    "    - Number of samples : m\n",
    "    - Image height : n_h\n",
    "    - Image width : n_w\n",
    "    - Number of channels : n_c ( 3 for rgb )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_padding(X, pad):\n",
    "    '''\n",
    "    Arguments:\n",
    "    X -- numpy array of shape (m, n_h, n_w, n_c) containing input images\n",
    "    pad -- amount of padding on the image\n",
    "    \n",
    "    Returns:\n",
    "    X_pad -- numpy array of shape (m, n_h + 2*pad, n_w + 2*pad, n_c) containing padded images\n",
    "    '''\n",
    "    X_pad = np.pad(X, ((0,0), (pad,pad), (pad,pad), (0,0)), mode = 'constant', constant_values = (0,0))\n",
    "    return X_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_step(Window, W, b):\n",
    "    '''\n",
    "    Arguments:\n",
    "    Window -- current window slice of the input image of shape (f, f, n_c_prev)\n",
    "    W -- weight matrix of shape (f, f, n_c_prev) where f is the filter size\n",
    "    b -- bias matrix of shape (1, 1, 1) where f is the filter size\n",
    "    \n",
    "    Returns:\n",
    "    Z -- convoluted output\n",
    "    '''\n",
    "    Z = np.multiply(Window, W)\n",
    "    Z = np.sum(Z)\n",
    "    Z = Z + float(b)\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_forward(A_prev, W, b, hyper_params):\n",
    "    '''\n",
    "    Arguments:\n",
    "    A_prev -- post-activation output of previous layer of shape (m, n_h_prev, n_w_prev, n_c_prev)\n",
    "    W -- weight matrix of shape (f, f, n_c_prev, n_c) where f is the filter size\n",
    "    b -- bias matrix of shape (1, 1, 1, n_c) where f is the filter size\n",
    "    hyper_params -- python dictionary containing strides and padding paramters\n",
    "    \n",
    "    Returns:\n",
    "    Z -- convoluted output of shape (m, n_h, n_w, n_c)\n",
    "    cache -- values required for backward propagation (A_prev, W, b, hyper_params)\n",
    "    '''\n",
    "    (m, n_h_prev, n_w_prev, n_c_prev) = A_prev.shape\n",
    "    (f, f, n_c_prev, n_c) = W.shape\n",
    "    \n",
    "    s = hyper_params['stride']\n",
    "    p = hyper_params['pad']\n",
    "    \n",
    "    n_h = int((n_h_prev + 2*p - f)/s) + 1\n",
    "    n_w = int((n_w_prev + 2*p - f)/s) + 1\n",
    "    \n",
    "    Z = np.zeros((m, n_h, n_w, n_c))\n",
    "    A_prev_pad = zero_padding(A_prev,p)\n",
    "    \n",
    "    for i in range(m):\n",
    "        curr_a_prev = A_prev_pad[i]\n",
    "        for h in range(n_h):\n",
    "            start_h = h*s\n",
    "            end_h = h*s + f\n",
    "            for w in range(n_w):\n",
    "                start_w = w*s\n",
    "                end_w = w*s + f\n",
    "                for c in range(n_c):\n",
    "                    Z[i,h,w,c] = conv_step(curr_a_prev[start_h:end_h,start_w:end_w,:], W[:,:,:,c], b[:,:,:,c])\n",
    "    cache = (A_prev, W, b, hyper_params)\n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pool_forward(A_prev, hyper_params, mode = 'max'):\n",
    "    '''\n",
    "    Arguments:\n",
    "    A_prev -- post-activation output of previous layer of shape (m, n_h_prev, n_w_prev, n_c_prev)\n",
    "    hyper_params -- python dictionary containing strides and padding paramters\n",
    "    mode -- type of the pooling function (max or average)\n",
    "    \n",
    "    Returns:\n",
    "    A -- pooled output of shape (m, n_h, n_w, n_c)\n",
    "    cache -- values required for backward propagation (A_prev, hyper_params)\n",
    "    '''\n",
    "    (m, n_h_prev, n_w_prev, n_c_prev) = A_prev.shape\n",
    "    \n",
    "    s = hyper_params['stride']\n",
    "    f = hyper_params['f']\n",
    "    \n",
    "    n_h = int((n_h_prev - f)/s) + 1\n",
    "    n_w = int((n_w_prev - f)/s) + 1\n",
    "    n_c = n_c_prev\n",
    "    A = np.zeros((m, n_h, n_w, n_c))\n",
    "    \n",
    "    for i in range(m):\n",
    "        curr_a_prev = A_prev[i]\n",
    "        for h in range(n_h):\n",
    "            start_h = h*s\n",
    "            end_h = h*s + f\n",
    "            for w in range(n_w):\n",
    "                start_w = w*s\n",
    "                end_w = w*s + f\n",
    "                for c in range(n_c):\n",
    "                    curr_window = curr_a_prev[start_h:end_h,start_w:end_w,c]\n",
    "                    \n",
    "                    if mode == 'max':\n",
    "                        A[i,h,w,c] = np.max(curr_window)\n",
    "                    elif mode == 'average':\n",
    "                        A[i,h,w,c] = np.mean(curr_window)\n",
    "    cache = (A_prev, hyper_params)\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_backward(dZ, cache):\n",
    "    '''\n",
    "    Arguments:\n",
    "    dZ -- gradient of cost with respect to the output of the layer (Z) of shape (m, n_h, n_w, n_c)\n",
    "    cache -- cache of values needed for the conv_backward(), output of conv_forward()\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- gradient of the cost with respect to the input of the conv layer (A_prev),\n",
    "               numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    dW -- gradient of the cost with respect to the weights of the conv layer (W)\n",
    "          numpy array of shape (f, f, n_C_prev, n_C)\n",
    "    db -- gradient of the cost with respect to the biases of the conv layer (b)\n",
    "          numpy array of shape (1, 1, 1, n_C)\n",
    "    '''\n",
    "    (m, n_h, n_w, n_c) = dZ.shape\n",
    "    (A_prev, W, b, hyper_params) = cache\n",
    "    (m, n_h_prev, n_w_prev, n_c_prev) = A_prev.shape\n",
    "    (f, f, n_c_prev, n_c) = W.shape\n",
    "    \n",
    "    s = hyper_params['stride']\n",
    "    p = hyper_params['pad']\n",
    "    \n",
    "    dA_prev = np.zeros((m, n_h_prev, n_w_prev, n_c_prev))\n",
    "    dW = np.zeros((f, f, n_c_prev, n_c))\n",
    "    db = np.zeros((1, 1, 1, n_c))\n",
    "    \n",
    "    A_prev_pad = zero_padding(A_prev, p)\n",
    "    dA_prev_pad = zero_padding(dA_prev, p)\n",
    "    \n",
    "    for i in range(m):\n",
    "        a_prev_pad = A_prev_pad[i]\n",
    "        da_prev_pad = dA_prev_pad[i]\n",
    "        for h in range(n_h):\n",
    "            start_h = h*s\n",
    "            end_h = h*s + f\n",
    "            for w in range(n_w):\n",
    "                start_w = w*s\n",
    "                end_w = w*s + f\n",
    "                for c in range(n_c):\n",
    "                    a_slice = a_prev_pad[start_h:end_h, start_w:end_w, :]\n",
    "                    da_prev_pad[start_h:end_h, start_w:end_w, :] += W[:, :, :, c]*dZ[i, h, w, c]\n",
    "                    dW[:, :, :, c] += a_slice*dZ[i, h, w, c]\n",
    "                    db[:, :, :, c] += dZ[i, h, w, c]\n",
    "        dA_prev[i, :, :, :] = da_prev_pad[p:-p, p:-p, :]\n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask(X):\n",
    "    '''\n",
    "    Arguments:\n",
    "    X -- Array of shape (f, f)\n",
    "    \n",
    "    Returns:\n",
    "    mask -- Array of the same shape as window, contains a True at the position corresponding to the max entry of x.\n",
    "    '''\n",
    "    mask = (X == np.max(X))\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distribute_value(dZ, shape):\n",
    "    '''\n",
    "    Arguments:\n",
    "    dZ -- input scalar\n",
    "    shape -- the shape (n_h, n_w) of the output matrix for which we want to distribute the value of dZ\n",
    "    \n",
    "    Returns:\n",
    "    A -- Array of size (n_h, n_w) for which we distributed the value of dZ\n",
    "    '''\n",
    "    (n_h, n_w) = shape\n",
    "    A = np.ones((n_h, n_w))*(dZ/(n_h*n_w))\n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pool_backward(dA, cache, mode = 'max'):\n",
    "    '''\n",
    "    Arguments:\n",
    "    dA -- gradient of cost with respect to the output of the pooling layer, same shape as A \n",
    "    cache -- cache output from the forward pass of the pooling layer, contains the layer's input and hparameters \n",
    "    mode -- pooling mode defined as a string ('max' or 'average')\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- gradient of cost with respect to the input of the pooling layer, same shape as A_prev\n",
    "    '''\n",
    "    A_prev, hyper_params = cache\n",
    "    m, n_h, n_w, n_c = dA.shape\n",
    "    m, n_h_prev, n_w_prev, n_c_prev = A_prev.shape\n",
    "    \n",
    "    s = hyper_params['stride']\n",
    "    f = hyper_params['f']\n",
    "    \n",
    "    dA_prev = np.zeros((m, n_h_prev, n_w_prev, n_c_prev))\n",
    "    \n",
    "    for i in range(m):\n",
    "        a_prev = A_prev[i]\n",
    "        for h in range(n_h):\n",
    "            start_h = h*s\n",
    "            end_h = h*s + f\n",
    "            for w in range(n_w):\n",
    "                start_w = w*s\n",
    "                end_w = w*s + f\n",
    "                for c in range(n_c):\n",
    "                    da = dA[i, h, w, c]\n",
    "                    if mode == 'max':\n",
    "                        mask = create_mask(A_prev[i, start_h:end_h, start_w:end_w, c])\n",
    "                        dA_prev[i, start_h:end_h, start_w:end_w, c] += np.multiply(mask, da)\n",
    "                    elif mode == 'average':\n",
    "                        dA_prev[i, start_h:end_h, start_w:end_w, c] += distribute_value(da, (f,f))\n",
    "    return dA_prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
